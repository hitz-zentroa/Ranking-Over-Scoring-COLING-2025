# Ranking Over Scoring: Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments

<p align="center">
<a href="https://coling2025.org/">
      <img alt="Web Page" src="https://img.shields.io/badge/COLING-Visit%20Here-red">
</a>
<a href="https://github.com/hitz-zentroa/cn-eval/blob/main/LICENSE">
        <img alt="GitHub license" src="https://img.shields.io/github/license/hitz-zentroa/cn-eval">
</a>
</p>



Welcome to the official repository for **"Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments"**, presented at COLING 2025. This repository aims to provide the scientific community with access to our models, proxy tasks, and tools developed during our research.


## Repository Structure

- **/data**
  - /Casimedicos_With_LLM_Argumentation_Mixture: This folder includes three training and development files, each containing a different mix of medical arguments generated by the LLMs used in our study: GPT-4, OpenBioLLM, and Llama 3.
  - /tests: This directory contains the JSONL files we used to test and evaluate our different evaluators.
  - /training: This directory includes example files for training and development, in case you want to train a new model.

- **/results**: This directory is designated for creating and storing results in JSONL format after training a new evaluator and performing inference with that evaluator.

- **/src**
  - /Inference: This folder contains all the necessary scripts to replicate the results of our paper in the MMCQA proxy task (referenced in Table 10) using the evaluator trained with medical arguments generated by Large Language Models (LLMs). Additionally, it includes scripts for evaluating new medical arguments with the models developed in our research.
  - /Training: This folder contains all the necessary scripts to train and evaluate a new model.
  - /Utils: This folder includes the script we used to balance the MMCQA datasets.

- `requirements.txt`: Lists the Python packages needed to run the project.
  
## Instructions for Use
To utilize the software provided in this repository, it is recommended to create a new virtual environment and install the necessary Python packages:
```bash
pip install -r requirements.txt
```
We also recommend that if you use a dataset not included in this repository, you first balance it using the `/generate_syn_dataset.py` script located in the `/src/Utils` folder. For this task set first the `/SETS_PATH` variable to the path where your original dataset files are located and set `/new_file_path` variable to the path where you want to save your balanced files.


### Replicating our results
In order to replicate the results presented in Table 10 of our paper, where we utilize the evaluator trained with different medical arguments generated by three Large Language Models (LLMs), download the pre-trained models from this link and organize your project directory by placing the downloaded `models/` directory alongside the existing `/data`, `/results`, and `/src` folders.

Next, navigate to the `src/Inference/` directory and execute the following command:

```bash
python3 test_Results_Replication.py
```

### Testing new medical arguments on MMCQA proxy task

If you wish to assess the quality of new medical arguments, place your JSONL file containing the new medical arguments into the `tests/` directory and execute the following command from the `src/Inference/` directory:

```bash
python3 test_New_Argumentation.py --test PathToTheTestFile.jsonl
```
Please ensure that you replace PathToTheTestFile.jsonl with the actual path to the test file you intend to evaluate. 

2. **LLM Evaluator**  

   For JudgeLM model evaluation, execute the following command:
   ```bash
   ./evaluation/bash/judge_full_pipeline.sh
  
For English, Spanish and Italian only the first line of the code should be edited. Specifically, modify the line `source ./.venv/bin/activate` to point to the path of your own environment. Additionally, you can change the variable `params` to adjust the size of the JudgeLM model. There are three available options: `7`, `13`, or `33`, referring to how many billion parameters the model has. Please note that each size will require a different amount of GPU memory.

For Basque, you should modify the line `source ./.venv/bin/activate` to point to the path of your own environment and **change the variable `--model-path` in the line of code 62 to `HiTZ/judge-eus`**. This is a 8B parameter judge model trained for Basque.

### Step 3: Analyze Results
Once the evaluations are complete, open the `Rank_models.ipynb` file in Jupyter Notebook. This notebook allows you to visualize the results and rank the models based on their performance. If you don't have Jupyter Notebook installed, you can easily run the notebook using Google Colab.

## Dependencies

This project requires the following dependencies:

- JudgeLM's dependencies are outlined in the [JudgeLM GitHub repository](https://github.com/baaivision/JudgeLM).

Additionally, you should execute the following command to install the required Python packages:

```bash
pip install -r requirements.txt
```

## References

```bibtex
@inproceedings{zubiaga2024llmbasedrankingmethodevaluation,
      title={A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation}, 
      author={Irune Zubiaga and Aitor Soroa and Rodrigo Agerri},
      year={2024},
      booktitle={Findings of EMNLP} 
}

@misc{zhu2023judgelmfinetunedlargelanguage,
      title={JudgeLM: Fine-tuned Large Language Models are Scalable Judges}, 
      author={Lianghui Zhu and Xinggang Wang and Xinlong Wang},
      year={2023},
      eprint={2310.17631},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.17631}, 
}

@inproceedings{ijcai2018p618,
  title     = {SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks},
  author    = {Ke Wang and Xiaojun Wan},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {4446--4452},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/618},
  url       = {https://doi.org/10.24963/ijcai.2018/618},
}

@misc{zhang2020bertscore,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
```
